"""
Whisper STT Server
Fast and accurate speech-to-text using faster-whisper
"""

import base64
import io
import time
from typing import Optional

import numpy as np
import soundfile as sf
from fastapi import FastAPI, HTTPException
from faster_whisper import WhisperModel
from pydantic import BaseModel

# ================================
# ì„¤ì •
# ================================
VERBOSE = True   # False: ìµœì†Œ ë¡œê·¸ë§Œ
DEBUG = True     # False: ìƒì„¸ ì •ë³´ ìˆ¨ê¹€

# ================================
# FastAPI ì•± ì´ˆê¸°í™”
# ================================
app = FastAPI(
    title="Whisper STT Server",
    description="Speech-to-Text using faster-whisper",
    version="1.0.0"
)

# ================================
# ì „ì—­ ë³€ìˆ˜
# ================================
models = {}  # ì–¸ì–´ë³„ ëª¨ë¸ ìºì‹œ (ì—¬ê¸°ì„œëŠ” ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš©)
device = "cpu"  # "cuda" or "cpu"
compute_type = "int8"  # "float16" (GPU) or "int8" (CPU)

# ================================
# ì–¸ì–´ ì½”ë“œ ë§¤í•‘
# ================================
LANGUAGE_MAP = {
    "KR": "ko",
    "EN": "en",
    "JP": "ja",
    "ZH": "zh",
    "FR": "fr",
    "DE": "de",
    "ES": "es",
    "RU": "ru",
}

# ================================
# ëª¨ë¸ ë¡œë”©
# ================================
def load_model():
    """Whisper ëª¨ë¸ ë¡œë“œ (base ëª¨ë¸ ì‚¬ìš© - tinyë³´ë‹¤ ì •í™•í•¨)"""
    global models
    
    if VERBOSE:
        print("=" * 60)
        print("ğŸš€ Whisper STT Server Starting...")
        print(f"â„¹ï¸  Device: {device}")
        print(f"â„¹ï¸  Compute Type: {compute_type}")
        print("=" * 60)
    
    try:
        if VERBOSE:
            print("ğŸ“¦ Loading Whisper base model...")
        
        start_time = time.time()
        
        # âœ… base ëª¨ë¸ ë¡œë“œ (tiny â†’ baseë¡œ ë³€ê²½í•˜ì—¬ ì •í™•ë„ í–¥ìƒ)
        # tiny: ê°€ì¥ ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ
        # base: ì†ë„ì™€ ì •í™•ë„ì˜ ê· í˜•
        # small/medium/large: ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼
        model = WhisperModel("base", device=device, compute_type=compute_type)
        models["default"] = model
        
        elapsed = time.time() - start_time
        
        if VERBOSE:
            print(f"âœ… Model loaded successfully in {elapsed:.2f}s")
            print("=" * 60)
            print("âœ… Server ready to transcribe speech!")
            print("=" * 60)
    
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        raise

# ================================
# API ëª¨ë¸
# ================================
class RecognizeRequest(BaseModel):
    audio_b64: str
    lang: str = "KR"
    sample_rate: int = 16000

class RecognizeResponse(BaseModel):
    text: str
    language: str
    segments: Optional[list] = None

class HealthResponse(BaseModel):
    status: str
    device: str
    model: str
    loaded_languages: list

# ================================
# API ì—”ë“œí¬ì¸íŠ¸
# ================================
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "ok",
        "device": device,
        "model": "base",  # tiny â†’ base
        "loaded_languages": list(LANGUAGE_MAP.keys())
    }

@app.post("/recognize", response_model=RecognizeResponse)
async def recognize(request: RecognizeRequest):
    """
    ìŒì„± ì¸ì‹ ìˆ˜í–‰
    
    Args:
        request: RecognizeRequest
            - audio_b64: Base64 ì¸ì½”ë”©ëœ WAV ì˜¤ë””ì˜¤
            - lang: ì–¸ì–´ ì½”ë“œ (KR, EN, JP, ZH, FR, DE, ES, RU)
            - sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ 16000)
    
    Returns:
        RecognizeResponse
            - text: ì¸ì‹ëœ í…ìŠ¤íŠ¸
            - language: ê°ì§€ëœ ì–¸ì–´
            - segments: ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ (ì„ íƒ)
    """
    if VERBOSE:
        print(f"\n{'='*60}")
        print(f"ğŸ¤ New recognition request: lang={request.lang}")
    
    start_time = time.time()
    
    try:
        # 1. Base64 ë””ì½”ë”©
        audio_bytes = base64.b64decode(request.audio_b64)
        
        if DEBUG:
            print(f"ğŸ“Š Audio size: {len(audio_bytes)} bytes")
        
        # 2. ì˜¤ë””ì˜¤ ë°ì´í„° ë¡œë“œ
        audio_data, sr = sf.read(io.BytesIO(audio_bytes))

        if DEBUG:
            print(f"ğŸ“Š Raw audio dtype: {audio_data.dtype}, shape: {audio_data.shape}")
            print(f"ğŸ“Š Raw audio info: sr={sr}Hz, duration={len(audio_data)/sr:.2f}s")

        # âœ… ìŠ¤í…Œë ˆì˜¤ â†’ ëª¨ë…¸ ë³€í™˜ (ì±„ë„ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° í‰ê· )
        if audio_data.ndim > 1:
            if DEBUG:
                print(f"ğŸ”„ Converting stereo ({audio_data.shape[1]} channels) to mono")
            audio_data = audio_data.mean(axis=1)

        # âœ… float32ë¡œ ìºìŠ¤íŒ… (WhisperëŠ” float32ë¥¼ ê¸°ëŒ€í•¨)
        if audio_data.dtype != np.float32:
            if DEBUG:
                print(f"ğŸ”„ Converting {audio_data.dtype} to float32")
            audio_data = audio_data.astype(np.float32)

        # âœ… ìƒ˜í”Œë ˆì´íŠ¸ í™•ì¸ (WhisperëŠ” ë‚´ë¶€ì ìœ¼ë¡œ 16kHzë¥¼ ì‚¬ìš©)
        if sr != 16000:
            if DEBUG:
                print(f"âš ï¸  Sample rate is {sr}Hz (Whisper expects 16kHz)")
                print(f"    Audio will be resampled internally by Whisper")

        if DEBUG:
            print(f"ğŸ“Š After convert: dtype={audio_data.dtype}, shape={audio_data.shape}")
            print(f"ğŸ“Š Audio stats: min={audio_data.min():.4f}, max={audio_data.max():.4f}, mean={audio_data.mean():.4f}")

        # 3. ì–¸ì–´ ì½”ë“œ ë³€í™˜
        whisper_lang = LANGUAGE_MAP.get(request.lang, "ko")
        
        if DEBUG:
            print(f"ğŸŒ Language: {request.lang} -> {whisper_lang}")
        
        # 4. ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        model = models.get("default")
        if model is None:
            raise HTTPException(status_code=500, detail="Model not loaded")
        
        # 5. ìŒì„± ì¸ì‹ ìˆ˜í–‰
        if DEBUG:
            print("ğŸ¯ Starting transcription...")
        
        transcribe_start = time.time()
        
        # âœ… ê°œì„ ëœ íŒŒë¼ë¯¸í„° ì„¤ì •
        segments, info = model.transcribe(
            audio_data,
            language=whisper_lang,
            vad_filter=True,           # Voice Activity Detection
            vad_parameters={
                "threshold": 0.5,      # VAD threshold (ë‚®ì„ìˆ˜ë¡ ë¯¼ê°)
                "min_speech_duration_ms": 250,
                "min_silence_duration_ms": 100,
            },
            beam_size=5,               # Beam search í¬ê¸°
            best_of=5,                 # í›„ë³´ ê°œìˆ˜
            temperature=0.0,           # ê²°ì •ì  ì¶œë ¥
            condition_on_previous_text=False,  # ì´ì „ í…ìŠ¤íŠ¸ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
            initial_prompt=None,       # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì—†ìŒ
            word_timestamps=False,     # ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ë¶ˆí•„ìš”
        )
        
        # 6. ê²°ê³¼ ìˆ˜ì§‘
        full_text = ""
        segment_list = []
        
        for segment in segments:
            full_text += segment.text
            segment_list.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            })
        
        transcribe_time = time.time() - transcribe_start
        
        if DEBUG:
            print(f"âœ… Transcription completed in {transcribe_time:.2f}s")
            print(f"ğŸ“ Detected language: {info.language} (probability: {info.language_probability:.2f})")
            print(f"ğŸ“ Number of segments: {len(segment_list)}")
            print(f"ğŸ“ Result: '{full_text.strip()}'")
        
        # âœ… ë¹ˆ ê²°ê³¼ ê²½ê³ 
        if not full_text.strip():
            if VERBOSE:
                print("âš ï¸  Warning: Empty transcription result")
                print("    This may happen if:")
                print("      - Audio is too quiet")
                print("      - Audio contains no speech")
                print("      - Audio quality is too poor")
        
        # 7. ì‘ë‹µ ìƒì„±
        total_time = time.time() - start_time
        
        if VERBOSE:
            print(f"âœ… Request completed in {total_time:.2f}s")
            if DEBUG:
                print(f"   Breakdown:")
                print(f"     Transcription: {transcribe_time:.2f}s")
            print("="*60)
        
        return RecognizeResponse(
            text=full_text.strip(),
            language=info.language,
            segments=segment_list if DEBUG else None
        )
    
    except Exception as e:
        if VERBOSE:
            print(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()
            print("="*60)
        raise HTTPException(status_code=500, detail=str(e))

# ================================
# ì„œë²„ ì‹œì‘ ì´ë²¤íŠ¸
# ================================
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()

# ================================
# ë©”ì¸
# ================================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8300)